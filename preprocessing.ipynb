{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7100a1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Filtering by 'dress' in title...\n",
      "Subset saved to fashion_subset_10000.json (10000 products)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# === CONFIG ===\n",
    "SOURCE_FILE = \"meta_Amazon_Fashion.jsonl\"  # your large dataset\n",
    "OUTPUT_FILE = \"fashion_subset_10000.json\"  # smaller subset for experiments\n",
    "CATEGORY_FILTER = \"dress\"            # keep only items with this word in title\n",
    "NUM_SAMPLES = 10000                   # number of products to keep\n",
    "KEEP_COLS = ['title', 'features', 'description', 'images', 'bought_together']\n",
    "\n",
    "# === LOAD DATA ===\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_json(SOURCE_FILE, lines=True)\n",
    "\n",
    "# === FILTER BY CATEGORY ===\n",
    "print(f\"Filtering by '{CATEGORY_FILTER}' in title...\")\n",
    "df = df[df['title'].str.contains(CATEGORY_FILTER, case=False, na=False)]\n",
    "\n",
    "# === KEEP ONLY NEEDED COLUMNS ===\n",
    "df = df[KEEP_COLS]\n",
    "\n",
    "# === DROP EMPTY FIELDS ===\n",
    "df = df.dropna(subset=['title', 'images', 'description'])\n",
    "\n",
    "# === SAMPLE A SMALLER SET ===\n",
    "if NUM_SAMPLES < len(df):\n",
    "    df = df.sample(n=NUM_SAMPLES, random_state=42)\n",
    "\n",
    "# === KEEP FIRST IMAGE ONLY (OPTIONAL) ===\n",
    "df['images'] = df['images'].apply(lambda x: x[0] if isinstance(x, list) and len(x) > 0 else None)\n",
    "\n",
    "# === SAVE SMALLER DATASET ===\n",
    "df.to_json(OUTPUT_FILE, orient=\"records\", lines=True)\n",
    "print(f\"Subset saved to {OUTPUT_FILE} ({len(df)} products)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d29ac106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model...\n",
      "Loading cached embeddings from image_embeddings.pt...\n",
      "[WARN] Cache file is not a dictionary. Ignoring and starting fresh.\n",
      "Found 10000 new images to process out of 10000 total.\n",
      "Downloading & encoding new images...\n",
      "Progress: 133/10000 images processed\n",
      "Progress: 339/10000 images processed\n",
      "Progress: 520/10000 images processed\n",
      "Progress: 688/10000 images processed\n",
      "Progress: 888/10000 images processed\n",
      "Progress: 1096/10000 images processed\n",
      "Progress: 1292/10000 images processed\n",
      "Progress: 1492/10000 images processed\n",
      "Progress: 1678/10000 images processed\n",
      "Progress: 1853/10000 images processed\n",
      "Progress: 2050/10000 images processed\n",
      "Progress: 2186/10000 images processed\n",
      "Progress: 2379/10000 images processed\n",
      "Progress: 2571/10000 images processed\n",
      "[WARN] Failed to process https://m.media-amazon.com/images/I/31DHbGX05ML._AC_.jpg (404 Client Error: Not Found for url: https://m.media-amazon.com/images/I/31DHbGX05ML._AC_.jpg)\n",
      "Progress: 2767/10000 images processed\n",
      "Progress: 2944/10000 images processed\n",
      "Progress: 3141/10000 images processed\n",
      "Progress: 3378/10000 images processed\n",
      "Progress: 3622/10000 images processed\n",
      "Progress: 3864/10000 images processed\n",
      "Progress: 4101/10000 images processed\n",
      "Progress: 4333/10000 images processed\n",
      "Progress: 4581/10000 images processed\n",
      "Progress: 4815/10000 images processed\n",
      "Progress: 4917/10000 images processed\n",
      "Progress: 5282/10000 images processed\n",
      "Progress: 5505/10000 images processed\n",
      "Progress: 5745/10000 images processed\n",
      "Progress: 5974/10000 images processed\n",
      "Progress: 6148/10000 images processed\n",
      "Progress: 6415/10000 images processed\n",
      "Progress: 6614/10000 images processed\n",
      "Progress: 6862/10000 images processed\n",
      "Progress: 7068/10000 images processed\n",
      "Progress: 7283/10000 images processed\n",
      "Progress: 7498/10000 images processed\n",
      "Progress: 7695/10000 images processed\n",
      "Progress: 7887/10000 images processed\n",
      "Progress: 8151/10000 images processed\n",
      "Progress: 8353/10000 images processed\n",
      "Progress: 8529/10000 images processed\n",
      "Progress: 8699/10000 images processed\n",
      "Progress: 8880/10000 images processed\n",
      "Progress: 9071/10000 images processed\n",
      "Progress: 9250/10000 images processed\n",
      "[WARN] Failed to process https://m.media-amazon.com/images/I/41ZpewU-AyL._AC_.jpg (HTTPSConnectionPool(host='m.media-amazon.com', port=443): Read timed out. (read timeout=5))\n",
      "Progress: 9350/10000 images processed\n",
      "[WARN] Failed to process https://m.media-amazon.com/images/I/31DHbGX05ML._AC_.jpg (404 Client Error: Not Found for url: https://m.media-amazon.com/images/I/31DHbGX05ML._AC_.jpg)\n",
      "Progress: 9650/10000 images processed\n",
      "Progress: 9912/10000 images processed\n",
      "Saving updated image embeddings cache...\n",
      "✅ Image preprocessing complete.\n"
     ]
    }
   ],
   "source": [
    "# preprocess_images.py\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "# ===============================\n",
    "# CONFIG\n",
    "# ===============================\n",
    "DATA_FILE = \"amazon_fashion_trimmed_10000.csv\"\n",
    "IMAGE_EMB_CACHE = \"image_embeddings.pt\"\n",
    "MODEL_NAME = \"clip-ViT-B-32\"\n",
    "NUM_THREADS = 20\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ===============================\n",
    "# Load dataset\n",
    "# ===============================\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "# ===============================\n",
    "# Load CLIP model\n",
    "# ===============================\n",
    "print(\"Loading CLIP model...\")\n",
    "model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
    "\n",
    "# Determine embedding dimension using a dummy image\n",
    "dummy_img = Image.new(\"RGB\", (224, 224), color=\"white\")\n",
    "embedding_dim = model.encode(dummy_img, convert_to_tensor=True).shape[-1]\n",
    "\n",
    "# ===============================\n",
    "# Load cache safely\n",
    "# ===============================\n",
    "cached_embeddings = {}\n",
    "if os.path.exists(IMAGE_EMB_CACHE):\n",
    "    print(f\"Loading cached embeddings from {IMAGE_EMB_CACHE}...\")\n",
    "    try:\n",
    "        loaded_cache = torch.load(IMAGE_EMB_CACHE, map_location=DEVICE)\n",
    "        if isinstance(loaded_cache, dict):\n",
    "            cached_embeddings = loaded_cache\n",
    "        else:\n",
    "            print(\"[WARN] Cache file is not a dictionary. Ignoring and starting fresh.\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to load cache: {e}. Starting fresh.\")\n",
    "\n",
    "# ===============================\n",
    "# Find which images need processing\n",
    "# ===============================\n",
    "urls = df[\"image_large\"].tolist()\n",
    "to_process = [url for url in urls if url not in cached_embeddings]\n",
    "print(f\"Found {len(to_process)} new images to process out of {len(urls)} total.\")\n",
    "\n",
    "# ===============================\n",
    "# Image processing function\n",
    "# ===============================\n",
    "def process_image(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        response.raise_for_status()\n",
    "        img = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "        emb = model.encode(img, convert_to_tensor=True)\n",
    "        return url, emb\n",
    "    except (UnidentifiedImageError, requests.RequestException, OSError) as e:\n",
    "        print(f\"[WARN] Failed to process {url} ({e})\")\n",
    "        return url, torch.zeros(embedding_dim)\n",
    "\n",
    "# ===============================\n",
    "# Process new images in parallel with progress printing\n",
    "# ===============================\n",
    "if to_process:\n",
    "    print(\"Downloading & encoding new images...\")\n",
    "    processed_count = 0\n",
    "    last_print_time = time.time()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:\n",
    "        for url, emb in executor.map(process_image, to_process):\n",
    "            cached_embeddings[url] = emb\n",
    "            processed_count += 1\n",
    "\n",
    "            # Print progress every 5 seconds\n",
    "            if time.time() - last_print_time >= 5:\n",
    "                print(f\"Progress: {processed_count}/{len(to_process)} images processed\")\n",
    "                last_print_time = time.time()\n",
    "\n",
    "# ===============================\n",
    "# Save updated cache\n",
    "# ===============================\n",
    "print(\"Saving updated image embeddings cache...\")\n",
    "torch.save(cached_embeddings, IMAGE_EMB_CACHE)\n",
    "\n",
    "print(\"✅ Image preprocessing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2740b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset (replace with your file path)\n",
    "df = pd.read_json(\"fashion_subset_10000.json\", lines=True)  # lines=True if it's JSON Lines format\n",
    "\n",
    "print(df.head())\n",
    "# Helper function to get large image URL\n",
    "def get_large_image(images):\n",
    "    if isinstance(images, dict):  # Direct dict case\n",
    "        return images.get(\"large\")\n",
    "    elif isinstance(images, list):  # List of dicts case\n",
    "        for img in images:\n",
    "            if isinstance(img, dict) and \"large\" in img:\n",
    "                return img[\"large\"]\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "# Create new dataframe with only needed columns\n",
    "df_trimmed = pd.DataFrame({\n",
    "    \"title\": df[\"title\"],\n",
    "    \"image_large\": df[\"images\"].apply(get_large_image)\n",
    "})\n",
    "\n",
    "# Drop rows without images or titles\n",
    "# df_trimmed = df_trimmed.dropna(subset=[\"image_large\", \"title\"]).reset_index(drop=True)\n",
    "\n",
    "# Save trimmed dataset for experiments\n",
    "df_trimmed.to_csv(\"amazon_fashion_trimmed_10000.csv\", index=False)\n",
    "\n",
    "print(f\"Trimmed dataset shape: {df_trimmed.shape}\")\n",
    "print(df_trimmed.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
