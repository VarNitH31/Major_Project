{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac6d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import torch\n",
    "# from PIL import Image\n",
    "# import requests\n",
    "# from io import BytesIO\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# # Load dataset\n",
    "# df = pd.read_csv(\"amazon_fashion_trimmed.csv\")\n",
    "\n",
    "# # Load CLIP model\n",
    "# model = SentenceTransformer(\"clip-ViT-B-32\")\n",
    "\n",
    "# # Generate embeddings\n",
    "# text_embeddings = model.encode(df[\"title\"].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# image_embeddings = []\n",
    "# for url in df[\"image_large\"]:\n",
    "#     try:\n",
    "#         img = Image.open(BytesIO(requests.get(url).content)).convert(\"RGB\")\n",
    "#         image_embeddings.append(model.encode(img, convert_to_tensor=True))\n",
    "#     except:\n",
    "#         image_embeddings.append(torch.zeros(text_embeddings.shape[1]))  # fallback\n",
    "\n",
    "# image_embeddings = torch.stack(image_embeddings)\n",
    "\n",
    "# # Fusion (equal weights here, can be dynamic later)\n",
    "# fused_embeddings = 0.5 * text_embeddings + 0.5 * image_embeddings\n",
    "# fused_embeddings = torch.nn.functional.normalize(fused_embeddings, p=2, dim=1)\n",
    "\n",
    "# # Recommendation function\n",
    "# def recommend(idx, top_k=5):\n",
    "#     query_emb = fused_embeddings[idx]\n",
    "#     scores = util.cos_sim(query_emb, fused_embeddings)[0]\n",
    "#     top_results = torch.topk(scores, k=top_k + 1)\n",
    "    \n",
    "#     print(f\"Query: {df.iloc[idx]['title']}\")\n",
    "#     for score, i in zip(top_results[0][1:], top_results[1][1:]):  # skip itself\n",
    "#         print(f\"{df.iloc[i]['title']} (score: {score:.4f})\")\n",
    "\n",
    "# # Example\n",
    "# recommend(0, top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444f1272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from PIL import Image\n",
    "# import requests\n",
    "# from io import BytesIO\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# import pickle\n",
    "\n",
    "# # ===============================\n",
    "# # CONFIG\n",
    "# # ===============================\n",
    "# DATA_FILE = \"amazon_fashion_trimmed.csv\"\n",
    "# TEXT_EMB_CACHE = \"text_embeddings.pt\"\n",
    "# IMAGE_EMB_CACHE = \"image_embeddings.pt\"\n",
    "# MODEL_NAME = \"clip-ViT-B-32\"\n",
    "# NUM_THREADS = 8  # Parallel image downloads\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# # ===============================\n",
    "# # Load dataset\n",
    "# # ===============================\n",
    "# df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "# # ===============================\n",
    "# # Load CLIP model\n",
    "# # ===============================\n",
    "# model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
    "\n",
    "# # ===============================\n",
    "# # Encode text (cached)\n",
    "# # ===============================\n",
    "# # if os.path.exists(TEXT_EMB_CACHE):\n",
    "# #     print(\"Loading cached text embeddings...\")\n",
    "# #     text_embeddings = torch.load(TEXT_EMB_CACHE, map_location=DEVICE)\n",
    "# # else:\n",
    "# #     print(\"Generating text embeddings...\")\n",
    "# #     text_embeddings = model.encode(df[\"title\"].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "# #     torch.save(text_embeddings, TEXT_EMB_CACHE)\n",
    "# print(\"Generating text embeddings...\")\n",
    "# text_embeddings = model.encode(df[\"title\"].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "\n",
    "# # ===============================\n",
    "# # Encode images in parallel (cached)\n",
    "# # ===============================\n",
    "# # def process_image(url):\n",
    "# #     try:\n",
    "# #         img = Image.open(BytesIO(requests.get(url, timeout=5).content)).convert(\"RGB\")\n",
    "# #         return model.encode(img, convert_to_tensor=True)\n",
    "# #     except Exception as e:\n",
    "# #         return torch.zeros(text_embeddings.shape[1])  # Fallback for failed images\n",
    "\n",
    "# # if os.path.exists(IMAGE_EMB_CACHE):\n",
    "# #     print(\"Loading cached image embeddings...\")\n",
    "# #     image_embeddings = torch.load(IMAGE_EMB_CACHE, map_location=DEVICE)\n",
    "# # else:\n",
    "# #     print(\"Downloading & encoding images in parallel...\")\n",
    "# #     with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:\n",
    "# #         image_embeddings_list = list(executor.map(process_image, df[\"image_large\"].tolist()))\n",
    "# #     image_embeddings = torch.stack(image_embeddings_list)\n",
    "# #     torch.save(image_embeddings, IMAGE_EMB_CACHE)\n",
    "\n",
    "# def process_image(url):\n",
    "#     try:\n",
    "#         img = Image.open(BytesIO(requests.get(url, timeout=5).content)).convert(\"RGB\")\n",
    "#         return model.encode(img, convert_to_tensor=True)\n",
    "#     except Exception:\n",
    "#         return torch.zeros(text_embeddings.shape[1])  # Fallback for failed images\n",
    "\n",
    "# print(\"Downloading & encoding images in parallel...\")\n",
    "# with ThreadPoolExecutor(max_workers=NUM_THREADS) as executor:\n",
    "#     image_embeddings_list = list(executor.map(process_image, df[\"image_large\"].tolist()))\n",
    "# image_embeddings = torch.stack(image_embeddings_list)\n",
    "\n",
    "\n",
    "# # ===============================\n",
    "# # Fusion (equal weight for now)\n",
    "# # ===============================\n",
    "# fused_embeddings = 0.5 * text_embeddings + 0.5 * image_embeddings\n",
    "# fused_embeddings = torch.nn.functional.normalize(fused_embeddings, p=2, dim=1)\n",
    "\n",
    "# # ===============================\n",
    "# # Recommendation function\n",
    "# # ===============================\n",
    "# def recommend(idx, top_k=5):\n",
    "#     query_emb = fused_embeddings[idx]\n",
    "#     scores = util.cos_sim(query_emb, fused_embeddings)[0]\n",
    "#     top_results = torch.topk(scores, k=top_k + 1)\n",
    "\n",
    "#     print(f\"\\nQuery: {df.iloc[idx]['title']}\")\n",
    "#     for score, i in zip(top_results[0][1:], top_results[1][1:]):  # skip itself\n",
    "#         i = int(i)  # Convert tensor to int\n",
    "#         print(f\"{df.iloc[i]['title']} (score: {score.item():.4f})\")\n",
    "\n",
    "# # ===============================\n",
    "# # Example usage\n",
    "# # ===============================\n",
    "# recommend(0, top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2f442c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display\n",
    "\n",
    "# def recommend(idx, top_k=5):\n",
    "#     query_emb = fused_embeddings[idx]\n",
    "#     scores = util.cos_sim(query_emb, fused_embeddings)[0]\n",
    "#     top_results = torch.topk(scores, k=top_k + 1)\n",
    "\n",
    "#     # Show the query item\n",
    "#     print(f\"\\nQuery: {df.iloc[idx]['title']}\")\n",
    "#     try:\n",
    "#         img = Image.open(BytesIO(requests.get(df.iloc[idx]['image_large']).content)).convert(\"RGB\")\n",
    "#         display(img)\n",
    "#     except:\n",
    "#         print(\"[Image not available]\")\n",
    "\n",
    "#     # Show recommendations\n",
    "#     print(\"\\nTop Recommendations:\")\n",
    "#     for score, i in zip(top_results[0][1:], top_results[1][1:]):  # skip itself\n",
    "#         i = int(i)  # Convert tensor to int\n",
    "#         print(f\"{df.iloc[i]['title']} (score: {score.item():.4f})\")\n",
    "#         try:\n",
    "#             img = Image.open(BytesIO(requests.get(df.iloc[i]['image_large']).content)).convert(\"RGB\")\n",
    "#             display(img)\n",
    "#         except:\n",
    "#             print(\"[Image not available]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a13071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# ===============================\n",
    "# CONFIG\n",
    "# ===============================\n",
    "DATA_FILE = \"amazon_fashion_trimmed_10000.csv\"\n",
    "TEXT_EMB_CACHE = \"text_embeddings.pt\"\n",
    "IMAGE_EMB_CACHE = \"image_embeddings.pt\"\n",
    "MODEL_NAME = \"clip-ViT-B-32\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ===============================\n",
    "# Load dataset\n",
    "# ===============================\n",
    "df = pd.read_csv(DATA_FILE)\n",
    "\n",
    "# ===============================\n",
    "# Load CLIP model\n",
    "# ===============================\n",
    "model = SentenceTransformer(MODEL_NAME, device=DEVICE)\n",
    "\n",
    "# ===============================\n",
    "# Load cached text embeddings\n",
    "# ===============================\n",
    "if os.path.exists(TEXT_EMB_CACHE):\n",
    "    print(\"Loading cached text embeddings...\")\n",
    "    text_embeddings = torch.load(TEXT_EMB_CACHE, map_location=DEVICE)\n",
    "else:\n",
    "    print(\"Generating text embeddings...\")\n",
    "    text_embeddings = model.encode(df[\"title\"].tolist(), convert_to_tensor=True, show_progress_bar=True)\n",
    "    torch.save(text_embeddings, TEXT_EMB_CACHE)\n",
    "\n",
    "# ===============================\n",
    "# Load cached image embeddings\n",
    "# ===============================\n",
    "if os.path.exists(IMAGE_EMB_CACHE):\n",
    "    print(\"Loading cached image embeddings...\")\n",
    "    img_cache = torch.load(IMAGE_EMB_CACHE, map_location=DEVICE)\n",
    "    \n",
    "    # If the cache is a dict, convert to tensor in the same order as df\n",
    "    if isinstance(img_cache, dict):\n",
    "        image_embeddings = torch.stack([img_cache[path] for path in df[\"image_large\"]])\n",
    "    else:\n",
    "        image_embeddings = img_cache\n",
    "else:\n",
    "    raise FileNotFoundError(f\"{IMAGE_EMB_CACHE} not found! Please generate and save it first.\")\n",
    "\n",
    "# ===============================\n",
    "# Fusion (equal weight for now)\n",
    "# ===============================\n",
    "fused_embeddings = 0.35 * text_embeddings + 0.65 * image_embeddings\n",
    "fused_embeddings = torch.nn.functional.normalize(fused_embeddings, p=2, dim=1)\n",
    "\n",
    "# ===============================\n",
    "# Recommendation function\n",
    "# ===============================\n",
    "def recommend(idx, top_k=5):\n",
    "    query_emb = fused_embeddings[idx]\n",
    "    scores = util.cos_sim(query_emb, fused_embeddings)[0]\n",
    "    top_results = torch.topk(scores, k=top_k + 1)\n",
    "\n",
    "    # Show the query item\n",
    "    print(f\"\\nQuery: {df.iloc[idx]['title']}\")\n",
    "    try:\n",
    "        img = Image.open(BytesIO(requests.get(df.iloc[idx]['image_large']).content)).convert(\"RGB\")\n",
    "        display(img)\n",
    "    except:\n",
    "        print(\"[Image not available]\")\n",
    "\n",
    "    # Show recommendations\n",
    "    print(\"\\nTop Recommendations:\")\n",
    "    for score, i in zip(top_results[0][1:], top_results[1][1:]):  # skip itself\n",
    "        i = int(i)  # Convert tensor to int\n",
    "        print(f\"{df.iloc[i]['title']} (score: {score.item():.4f})\")\n",
    "        try:\n",
    "            img = Image.open(BytesIO(requests.get(df.iloc[i]['image_large']).content)).convert(\"RGB\")\n",
    "            display(img)\n",
    "        except:\n",
    "            print(\"[Image not available]\")\n",
    "\n",
    "# ===============================\n",
    "# Example usage\n",
    "# ===============================\n",
    "# recommend(0, top_k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e2c13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend(8682, top_k=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
